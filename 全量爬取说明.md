# 全量爬取说明

## 问题说明

之前的代码存在一个误解：**全量爬取被限制为 1000 条**。

实际上，"全量"应该是指**所有历史数据**，而不是限制在 1000 条。

## 修复内容

### Python 版本

**修改前**：
```python
def crawl_all(self, max_pages: int = None, use_api_first: bool = True):
    if use_api_first:
        api_data = self.fetch_api_recent(max_count=1000)
        if not max_pages or max_pages == 0:
            return api_data  # ❌ 直接返回 1000 条，不继续爬取
```

**修改后**：
```python
def crawl_all(self, max_pages: int = None, use_api_first: bool = False):
    # use_api_first 默认改为 False
    if use_api_first:
        api_data = self.fetch_api_recent(max_count=1000)
        all_data.extend(api_data)  # ✅ 作为起点，继续分页爬取
        logger.info("继续分页爬取更早的历史数据...")
    
    # 继续分页爬取所有数据
    while True:
        # ... 分页逻辑
```

**关键改进**：
1. `use_api_first` 默认改为 `False`（避免误用）
2. API 获取的数据不再直接返回，而是作为起点
3. 继续分页爬取所有历史数据
4. `max_pages=None` 表示真正的全量（所有页）

### Cloudflare Worker 版本

**修改前**：
```javascript
async fetchAll(maxCount = 1000) {
  // ❌ 默认只获取 1000 期
  return await this.fetchAllFromZhcw(maxCount);
}
```

**修改后**：
```javascript
async fetchAll(maxCount = null) {
  // ✅ maxCount=null 表示获取所有数据
  return await this.fetchAllFromZhcw(maxCount);
}

async fetchAllFromZhcw(maxCount = null) {
  const requestCount = maxCount || 10000; // 默认请求 10000 期（足够覆盖所有历史）
  // ...
}
```

**关键改进**：
1. `maxCount` 默认改为 `null`（表示不限制）
2. 内部默认请求 10000 期（足够覆盖双色球所有历史）
3. `/init` 接口不再限制为 1000 期

## 使用方式

### Python 版本

#### 方式 1：真正的全量爬取（推荐）

```python
from lotteries.ssq.spider import SSQSpider

spider = SSQSpider()

# 爬取所有历史数据（不限制页数）
all_data = spider.crawl_all(max_pages=None, use_api_first=False)
print(f"共获取 {len(all_data)} 条历史数据")
```

#### 方式 2：API + 分页（更快）

```python
# 先用 API 快速获取最近 1000 期，然后继续分页爬取更早的数据
all_data = spider.crawl_all(max_pages=None, use_api_first=True)
```

#### 方式 3：限制页数（测试用）

```python
# 只爬取前 10 页（用于测试）
test_data = spider.crawl_all(max_pages=10, use_api_first=False)
```

### Cloudflare Worker 版本

#### 方式 1：分批模式（推荐，避免超时）

```bash
# 多次触发 /run，每次爬取 100 期
# 系统会自动检测并继续爬取，直到数据完整

for i in {1..50}; do
  echo "执行第 $i 次..."
  curl -X POST https://your-worker.workers.dev/run \
    -H "Authorization: Bearer YOUR_API_KEY"
  sleep 120
done
```

#### 方式 2：一次性获取（可能超时）

```bash
# 默认：尽可能多地获取数据
curl -X POST https://your-worker.workers.dev/init \
  -H "Authorization: Bearer YOUR_API_KEY"

# 指定数量：获取最近 5000 期
curl -X POST "https://your-worker.workers.dev/init?count=5000" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

⚠️ **注意**：Cloudflare Worker 有 CPU 时间限制（约 30 秒），一次性获取大量数据可能超时。

## 双色球历史数据量

截至 2025 年 11 月：
- 双色球从 **2003 年 2 月 16 日**开始发行
- 每周开奖 **3 次**（周二、周四、周日）
- 累计约 **4000+ 期**

因此：
- **1000 期** ≈ 最近 6-7 个月的数据
- **4000 期** ≈ 所有历史数据

## API 限制说明

### 中彩网 API

- **单次请求**：最多返回 1000 个期号
- **解决方案**：
  - Python：获取 1000 期后，继续分页爬取更早的数据
  - Cloudflare Worker：分批模式，每次 100 期

### 500.com（备用源）

- **限制**：只返回最近 30 期
- **用途**：仅用于日常更新，不适合全量爬取

## 最佳实践

### 首次初始化（获取所有历史数据）

**Python 版本**：
```python
# 一次性获取所有数据（可能需要几分钟）
spider = SSQSpider()
all_data = spider.crawl_all(max_pages=None, use_api_first=False)

# 保存到数据库
# ...
```

**Cloudflare Worker 版本**：
```bash
# 使用分批模式，多次触发（推荐）
./cloudflare-worker/init.sh

# 或手动触发多次
for i in {1..50}; do
  curl -X POST https://your-worker.workers.dev/run \
    -H "Authorization: Bearer YOUR_API_KEY"
  sleep 120
done
```

### 日常更新（获取最新数据）

**Python 版本**：
```python
# 只获取最新 1 期
spider = SSQSpider()
latest = spider.fetch_latest(count=1)
```

**Cloudflare Worker 版本**：
```bash
# 定时任务自动执行，或手动触发
curl -X POST https://your-worker.workers.dev/run \
  -H "Authorization: Bearer YOUR_API_KEY"
```

## 性能对比

| 方式 | 数据量 | 耗时 | 适用场景 |
|------|--------|------|----------|
| Python 全量爬取 | 4000+ 期 | 5-10 分钟 | 首次初始化 |
| Python API + 分页 | 4000+ 期 | 3-5 分钟 | 首次初始化（更快） |
| Python 增量更新 | 1-10 期 | 10-30 秒 | 日常更新 |
| Cloudflare Worker 分批 | 100 期/次 | 1-2 分钟/次 | 首次初始化（避免超时） |
| Cloudflare Worker 增量 | 1-10 期 | 30 秒 | 日常更新 |

## 总结

1. **全量 = 所有历史数据**，不是 1000 条
2. **Python 版本**：可以一次性获取所有数据
3. **Cloudflare Worker 版本**：建议分批获取（避免超时）
4. **日常更新**：两个版本都支持智能增量更新

---

**修复完成时间**：2025-11-17  
**修复内容**：移除 1000 条限制，支持真正的全量爬取
